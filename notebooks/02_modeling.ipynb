{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f40a3f27-1a8e-4e6a-ab97-9c90c92552a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162819, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is awesome to listen to, A must-have for ...</td>\n",
       "      <td>Slayer Rules!</td>\n",
       "      <td>1</td>\n",
       "      <td>awesome listen musthave slayer fanssadly neede...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bien</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1</td>\n",
       "      <td>bien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was great to hear the old stuff again and I...</td>\n",
       "      <td>SLAYER!!!!!!!!!!!!!!!!!!!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>great hear old stuff like new stuff recommend ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>well best of's are a bit poison normally but t...</td>\n",
       "      <td>slayer greatest hits! you mean everything righ...</td>\n",
       "      <td>1</td>\n",
       "      <td>well best ofs bit poison normally bad pretty g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What can I say? This is Casting Crowns!!!This ...</td>\n",
       "      <td>This is a good, blessing filled</td>\n",
       "      <td>1</td>\n",
       "      <td>say casting crownsthis good blessing filled cd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  \\\n",
       "0  This is awesome to listen to, A must-have for ...   \n",
       "1                                               bien   \n",
       "2  It was great to hear the old stuff again and I...   \n",
       "3  well best of's are a bit poison normally but t...   \n",
       "4  What can I say? This is Casting Crowns!!!This ...   \n",
       "\n",
       "                                             summary  sentiment  \\\n",
       "0                                      Slayer Rules!          1   \n",
       "1                                         Five Stars          1   \n",
       "2                        SLAYER!!!!!!!!!!!!!!!!!!!!!          1   \n",
       "3  slayer greatest hits! you mean everything righ...          1   \n",
       "4                    This is a good, blessing filled          1   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  awesome listen musthave slayer fanssadly neede...  \n",
       "1                                               bien  \n",
       "2  great hear old stuff like new stuff recommend ...  \n",
       "3  well best ofs bit poison normally bad pretty g...  \n",
       "4     say casting crownsthis good blessing filled cd  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/cleaned_reviews.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f2dd57-abc2-4db8-b48a-18e376d1d584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: (130255,)\n",
      "Testing size: (32564,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['cleaned_text']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training size:\", X_train.shape)\n",
    "print(\"Testing size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5055674a-7114-4e39-ab9e-29603ec68533",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[32m      3\u001b[39m vectorizer = TfidfVectorizer(max_features=\u001b[32m10000\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m X_train_tfidf = \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m X_test_tfidf = vectorizer.transform(X_test)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining matrix shape:\u001b[39m\u001b[33m\"\u001b[39m, X_train_tfidf.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\sentiment-analysis-api\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\sentiment-analysis-api\\venv\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\sentiment-analysis-api\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1386\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1378\u001b[39m             warnings.warn(\n\u001b[32m   1379\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1380\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1381\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1382\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1383\u001b[39m             )\n\u001b[32m   1384\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1389\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\sentiment-analysis-api\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1273\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[32m   1272\u001b[39m     feature_counter = {}\n\u001b[32m-> \u001b[39m\u001b[32m1273\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1274\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1275\u001b[39m             feature_idx = vocabulary[feature]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\sentiment-analysis-api\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:108\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[33;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[32m     88\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    104\u001b[39m \u001b[33;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     doc = \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    110\u001b[39m     doc = analyzer(doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\sentiment-analysis-api\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:241\u001b[39m, in \u001b[36m_VectorizerMixin.decode\u001b[39m\u001b[34m(self, doc)\u001b[39m\n\u001b[32m    238\u001b[39m     doc = doc.decode(\u001b[38;5;28mself\u001b[39m.encoding, \u001b[38;5;28mself\u001b[39m.decode_error)\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np.nan:\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    242\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m     )\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[31mValueError\u001b[39m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"Training matrix shape:\", X_train_tfidf.shape)\n",
    "print(\"Testing matrix shape:\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "496cbdcb-c75c-497c-8e61-3c58601c76d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training matrix shape: (129363, 10000)\n",
      "Testing matrix shape: (32341, 10000)\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['cleaned_text'])\n",
    "\n",
    "X = df['cleaned_text']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"Training matrix shape:\", X_train_tfidf.shape)\n",
    "print(\"Testing matrix shape:\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb3b819a-e386-41ba-9f72-cba6e049dbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.25      0.39       743\n",
      "           1       0.98      1.00      0.99     31598\n",
      "\n",
      "    accuracy                           0.98     32341\n",
      "   macro avg       0.92      0.62      0.69     32341\n",
      "weighted avg       0.98      0.98      0.98     32341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40222ac1-4816-44e0-b155-e6e18292fcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.83      0.35       743\n",
      "           1       1.00      0.93      0.96     31598\n",
      "\n",
      "    accuracy                           0.93     32341\n",
      "   macro avg       0.61      0.88      0.65     32341\n",
      "weighted avg       0.98      0.93      0.95     32341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_balanced = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model_balanced.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_balanced = model_balanced.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred_balanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eb3619f-10c0-45df-aa2b-efc6d9cb75b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "1    3986\n",
      "0    3986\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate classes\n",
    "df_positive = df[df['sentiment'] == 1]\n",
    "df_negative = df[df['sentiment'] == 0]\n",
    "\n",
    "# Undersample positive to match negative count\n",
    "df_positive_undersampled = resample(df_positive, \n",
    "                                     n_samples=len(df_negative),\n",
    "                                     random_state=42)\n",
    "\n",
    "# Combine\n",
    "df_balanced = pd.concat([df_positive_undersampled, df_negative])\n",
    "print(df_balanced['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9cb64ea-160e-4000-9fb7-cb50cea8580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87       783\n",
      "           1       0.90      0.83      0.87       812\n",
      "\n",
      "    accuracy                           0.87      1595\n",
      "   macro avg       0.87      0.87      0.87      1595\n",
      "weighted avg       0.87      0.87      0.87      1595\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_balanced = df_balanced['cleaned_text']\n",
    "y_balanced = df_balanced['sentiment']\n",
    "\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer_b = TfidfVectorizer(max_features=10000)\n",
    "X_train_b_tfidf = vectorizer_b.fit_transform(X_train_b)\n",
    "X_test_b_tfidf = vectorizer_b.transform(X_test_b)\n",
    "\n",
    "model_b = LogisticRegression(max_iter=1000)\n",
    "model_b.fit(X_train_b_tfidf, y_train_b)\n",
    "\n",
    "y_pred_b = model_b.predict(X_test_b_tfidf)\n",
    "print(classification_report(y_test_b, y_pred_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea5994a5-0614-4f9e-8519-519622d5d00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer saved!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model_b, '../model/sentiment_model.pkl')\n",
    "joblib.dump(vectorizer_b, '../model/vectorizer.pkl')\n",
    "\n",
    "print(\"Model and vectorizer saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab4e75-0432-4438-9139-91ed0c5e4fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
